{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context('poster')\n",
    "\n",
    "colors = sns.color_palette(\"deep\", 10)\n",
    "sns.set_palette(colors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions: Neural Networks and Deep Learning\n",
    "\n",
    "In this notebook, we will scratch the surface of using neural networks and deep learning. We will mostly use the one model for this that is implemented in scikit-learn (at the time of writing this workshop), because then we are not distracted by interfacing and API issues and can focus on what we are trying to do. In the instruction notebook, there is an overview of how to solve one of the last of the exercises here with Tensorflow, a widely used general-purpose package for deep neural networks. Other often used packages are theano and keras. For all three, and for doing machine learning in general, the law of conservation of misery holds: if you want to be able to tweak more and be more flexible, it will come at the cost of a more complicated interface, for which you will have to specify more manually.\n",
    "\n",
    "## The MNIST handwritten digits\n",
    "\n",
    "There is a famous data set of hand written digits that are labeled. It's called MNIST and is famous enough to have a fetcher function in sklearn.datasets. We will first work with a smaller version of the data set, that came with the workshop material:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digitfile = 'data/digits_train_sample.csv'\n",
    "\n",
    "digits = pd.read_csv(digitfile)\n",
    "digits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPEAAAHBCAYAAAC4xl0QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb11f62a240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You might notice that there are 784 pixels, which happens to be 28**2, which is the shape of the images\n",
    "imsize = 28\n",
    "\n",
    "# Let's plot a random example of each one - Running this a few times will give you different images each time! \n",
    "plt.figure(figsize=[5, 8])\n",
    "for target in range(10):\n",
    "    deze = np.random.choice(digits[digits.label == target].index)\n",
    "    number = np.reshape(np.array(digits[digits.index == deze])[0][1:], [imsize, imsize])\n",
    "    pp = plt.subplot(5, 2, target+1)\n",
    "    pp.imshow(number, cmap='Greys')\n",
    "    pp.set_xticklabels([])\n",
    "    pp.set_yticklabels([])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some good practice with pandas-like data sets, please create similar thumbnail images of all digits, now showing the average image per label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's transform the pandas data frames into the data structures that scikit-learn likes more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = digits.label\n",
    "features = digits.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So, with all the experience you have by now, you should be able to run a quick logistic regression model. It should predict the labels given the features. Score on both training and test data and have a look at the confusion matrix (a matrix of predicted label versus real label), make a plot of that and see if you can to an extent explain the appearance of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The logistic regression does a decent job on the test sample, but seems to be overfitting. \n",
    "\n",
    "*Question 1*: What does that mean, and how can you see that from the above results?\n",
    "\n",
    "*Question 2*: Why would you not necessarily expect a logistic regression to perform extremely well on a problem like this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the arsenal of methods we have seen before, we can try to see if there is any structure in the data anyway.  Try a PCA, as well as a 2-component t-SNE to see if there is structure in the data. For the PCA case: can you identify the two principle components in vague descriptions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Alright, let's do neural networks. Neural networks do better with bigger data sets. Therefore, we will use the data fetcher and get a bigger data set of digits, that apart from the data structure it is in, is extremely similar to the data used above. Now that you have seen the fetcher before, try to figure how to do this yourself. If necessary, use google, the docs, etc. If all else fails, there's always the solutions notebook.\n",
    "\n",
    "So, go ahead and fetch the data, and create an array of features and labels in the same units as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently, create a train and test set and in order to have a fair comparison to the exercise above, run a logistic regression on this data as well. Think about the difference in results for a bit. Why is $R^2$ significantly higher with this bigger data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, onto the neural networks. sklearn.neural_network as the Multi-layer Perceptron model, which can do traditional, as well as deep neural networks. Have a look at the docs.\n",
    "\n",
    "Set up a MLPClassifier for the digits with one layer. Score on train and test data. Then play with the number of neurons of the one hidden layer, vary it strongly (go below ten, as well as far above) and investigate the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now train a deep network, with three layers. You should be able to get the $R^2$ close to 98%. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-encoding\n",
    "\n",
    "Deep networks, with layers halfway that consist of few neurons can be used as a dimension reduction tool. After all, if the score fo the full network is X, all that information reaches the output, *through* that layer of few neurons, which, therefore, must contain at least contain the information to recover a fraction X of all true labels.\n",
    "\n",
    "These images contain 784 pixels, but as it will turn out, a model score of 95% can be reached with a deep \"auto-encoding\" neural network that has a middle layer of just 2 neurons! One way to reduce data volumes would be to keep only 2 numbers (the output value of the layer of two neurons) and the weights for the network that goes back up to the output scheme.\n",
    "\n",
    "Train an auto-encoder, that has at least one layer, somehwere, with almost 2 neurons. Get the score on the output well above 90% (on unseen data). When you get that working, use the .coefs\\_ attribute and their documentation to find output of the layer of 2 neurons and plot those two \"components\" as a scatter plot. Discuss with yourself and neighbors what you could do with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bonus exercise\n",
    "\n",
    "There is no recipe. There is no cookbook. Play around (either by hand, or with grid search) with some hyperparameters and see if you can get a better score (also without the auto-encoding). Typically, to get over 99% (and, therefore, to even remotely have a chance to compete with pride on Kaggle) you will have to do feature engineering. The images are not in their best shape as they are. You could sit here for another few days and try stuff. Why don't you try something you like?\n",
    "\n",
    "** * Only read on if you want a spoiler-like hint * **\n",
    "\n",
    "If you can't think of anything yourself, here is a suggestion (one that actually should work well, if you do it well). The digits have empty space surrounding them on the images, both vertically and horizontally. If you remove all empty columns and rows, you will end up with images of different shapes (especially those with a 1 will be quite narrow). Reshape (interpolate, whatever you seem fit) to get them back to the original size (ones will become mostly black) them back to the original size and fit another neural network (deep, with many neurons in the first hidden layer) on this data set. That should already help. If you want even more, you could think of an \"orientation correction\", that first gets all images aligned, which helps especially for narrow images again.\n",
    "\n",
    "There are no solutions to this, because there is way too much freedom in here. For inspiration and further practice, see [the kaggle page of this data set](https://www.kaggle.com/c/digit-recognizer/kernels). In the instruction notebook there is an example of doing something like this using TensorFlow, which is more versatile, but also more difficult to use, than what is done here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
